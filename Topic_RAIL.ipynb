{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# load rails config\n",
    "config = RailsConfig.from_path(\"./topic_rail_config/openai\")\n",
    "\n",
    "# create rails\n",
    "rails = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you?\n",
      "How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Hey there!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "However, I can help you find the latest soccer merchandise or equipment. Is there anything specific you are looking for?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Tell me about the latest soccer match.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "However, I can help you with finding and purchasing any products you need.\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Do you know who is Lionel Messi?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails with HuggingFace Pipeline - Llamav2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sofiaperez/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    logprobs=None,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./topic_rail_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "rails = LLMRails(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "How are you today?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"hello\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Tell me about the latest soccer match.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Do you know who is Lionel Messi?\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
