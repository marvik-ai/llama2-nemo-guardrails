{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Guidance\n",
    "\n",
    "The idea behind this use case is to guide the model to stick to certain topics and avoid answering certain questions. In other words, the Guardrail layer examines every user input and filters them accordingly. \n",
    "\n",
    "In this notebook we will implement a simple example where the chatbot is instructed not to answer any sport-related questions. We will use both OpenAI and Llama2 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load environment \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails with OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Guardrails configuration files located under `topic_rail_config/openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "## Load rails config\n",
    "config = RailsConfig.from_path(\"./topic_rail_config/openai\")\n",
    "\n",
    "## Create rails\n",
    "rails = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passed a typical greeting, protective guardrails are not activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you?\n",
      "How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Hey there!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked about a match result or a football player, the chatbot effectively evades the question as designed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "However, I can help you find the latest soccer merchandise or equipment. Is there anything specific you are looking for?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Tell me about the latest soccer match.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "However, I can help you with finding and purchasing any products you need.\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Do you know who is Lionel Messi?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails with Llama2\n",
    "\n",
    "Nemo Guardrails also supports loading models from HuggingFace hub. We will use this to load Llama2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the HuggingFace model and create a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sofiaperez/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.90s/it]\n"
     ]
    }
   ],
   "source": [
    "## Load hugging face credentials\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
    "\n",
    "## Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    logprobs=None,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the pipeline using Langchain HuggingFacePipeline class. Then wrap it again using Nemo’s get_llm_instance_wrapper function and register it using register_llm_provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Guardrails configuration files located under `topic_rail_config/llama2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "## Initialize rails config\n",
    "config = RailsConfig.from_path(\"./topic_rail_config/llama2\")\n",
    "\n",
    "## Create rails\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passed a typical greeting, protective guardrails are not activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "How are you today?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"hello\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked about a match result or a football player, the chatbot effectively evades the question as designed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Tell me about the latest soccer match.\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of sports.\n",
      "Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Do you know who is Lionel Messi?\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
