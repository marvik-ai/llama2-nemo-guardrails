{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiaperez/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./hallucination_config/openai\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NVIDIA GeForce RTX 4090 has 5,376 CUDA cores.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"How many CUDA cores does a 4090 have?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the hallucination rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./hallucination_config/openai\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NVIDIA GeForce RTX 4090 has 8704 CUDA cores. However, this may be subject to change depending on the model and manufacturer.\n",
      "The previous answer is prone to hallucination and may not be accurate. Please double check the answer using additional sources.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"How many CUDA cores does a 4090 have?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiaperez/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sofiaperez/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    logprobs=None,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./hallucination_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n",
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 4090 GPU has 88 RT cores, 46 Tensor cores, and 160 CUDA cores.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"How many CUDA cores does a 4090 have?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the hallucination rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./hallucination_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n",
      "Parameter temperature does not exist for WrapperLLM\n",
      "Hallucination rail can only be used with OpenAI LLM engines.Current LLM engine is WrapperLLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination rail can only be used with OpenAI LLM engines.Current LLM engine is WrapperLLM.\n",
      "According to my knowledge base, the Nvidia GeForce RTX 4090 has 48 CUDA cores.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"How many CUDA cores does a 4090 have?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing hallucination.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    logprobs=None,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./hallucination_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms.base import BaseLLM\n",
    "\n",
    "from nemoguardrails.actions.llm.utils import (\n",
    "    get_multiline_response,\n",
    "    llm_call,\n",
    "    strip_quotes,\n",
    ")\n",
    "from nemoguardrails.llm.params import llm_params\n",
    "from nemoguardrails.llm.taskmanager import LLMTaskManager\n",
    "from nemoguardrails.llm.types import Task\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "HALLUCINATION_NUM_EXTRA_RESPONSES = 2\n",
    "\n",
    "\n",
    "async def check_hallucination_llama(\n",
    "    llm_task_manager: LLMTaskManager,\n",
    "    context: Optional[dict] = None,\n",
    "    llm: Optional[BaseLLM] = None,\n",
    "    use_llm_checking: bool = True,\n",
    "):\n",
    "    \"\"\"Checks if the last bot response is a hallucination by checking multiple completions for self-consistency.\n",
    "\n",
    "    :return: True if hallucination is detected, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    bot_response = context.get(\"last_bot_message\")\n",
    "    last_user_message_string = context.get(\"last_user_message\")\n",
    "    num_responses = HALLUCINATION_NUM_EXTRA_RESPONSES\n",
    "    \n",
    "    # Use the \"generate\" call from langchain to get all completions in the same response.\n",
    "    last_bot_prompt = PromptTemplate(template=\"{text}\", input_variables=[\"text\"])\n",
    "    chain = LLMChain(prompt=last_bot_prompt, llm=llm)\n",
    "    \n",
    "    extra_responses = []\n",
    "    for i in range(num_responses):\n",
    "        result = chain.run(last_user_message_string)    \n",
    "        result = get_multiline_response(result)\n",
    "        result = strip_quotes(result)\n",
    "        extra_responses.append(result)\n",
    "    \n",
    "    if len(extra_responses) == 0:\n",
    "        print(f\"No extra LLM responses were generated for '{bot_response}' hallucination check.\")\n",
    "        return False\n",
    "    elif len(extra_responses) < num_responses:\n",
    "        print(f\"Requested {num_responses} extra LLM responses for hallucination check, \"\n",
    "            f\"received {len(extra_responses)}.\")\n",
    "        \n",
    "    if use_llm_checking:\n",
    "        # Only support LLM-based agreement check in current version\n",
    "        prompt = llm_task_manager.render_task_prompt(\n",
    "            task=Task.CHECK_HALLUCINATION,\n",
    "            context={\n",
    "                \"statement\": bot_response,\n",
    "                \"paragraph\": \" \".join(extra_responses),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        with llm_params(llm):#, temperature=0.0):\n",
    "            agreement = await llm_call(llm, prompt)\n",
    "\n",
    "        agreement = agreement.lower().strip()\n",
    "        print(f\"Agreement result for looking for hallucination is {agreement}.\")\n",
    "        # Return True if the hallucination check fails\n",
    "        return \"no\" in agreement\n",
    "\n",
    "    return False\n",
    "\n",
    "app.register_action(check_hallucination_llama, name=\"check_hallucination_llama\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n",
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement result for looking for hallucination is no.\n",
      "A GeForce RTX 4090 has 24 GB of GDDR6X memory and 5888 CUDA cores.\n",
      "The previous answer is prone to hallucination and may not be accurate. Please double check the answer using additional sources.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"How many CUDA cores does a 4090 have?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
