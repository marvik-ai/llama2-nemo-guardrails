{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiaperez/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/openai\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the US Bureau of Labor Statistics, the unemployment rate in March 2023 was 3.5 percent.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"What was the unemployment rate in March 2023?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the US Bureau of Labor Statistics, the unemployment rate for teenagers in March 2023 was 9.8 percent.\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for teenagers?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the US Bureau of Labor Statistics, the unemployment rate for senior citizens in March 2023 was 4.7 percent.\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for senior citizens?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the fact check rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/openai\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the March 2023 US jobs report, the unemployment rate in March 2023 was 3.5 percent.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"What was the unemployment rate in March 2023?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the March 2023 US jobs report, the unemployment rate for teenagers was 9.8 percent.\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for teenagers?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough information to answer\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for senior citizens?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiaperez/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sofiaperez/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    logprobs=None,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the latest US jobs report, the unemployment rate in March 2023 was 3.5 percent.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate in March 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the most recent data from the Bureau of Labor Statistics, the unemployment rate for seniors (aged 65 and older) was 3.2% in March 2023. However, I must inform you that this figure may not be accurate due to various factors such as underreporting or misclassification of workers. If you have any further questions or concerns, please feel free to ask.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate for senior citizens?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the fact check rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate in March 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the fact check rail + few shot in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate for senior citizens?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
