{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact-Checking Guardrails\n",
    "\n",
    "This use case consists of asking the LLM to re-check whether the output is consistent with the context. In other words, ask the LLM if the response is true based on the documents retrieved from the knowledge base.\n",
    "\n",
    "This notebook is based on example **Grounding Rail** presented in Nemo Guardrails official [repo](https://github.com/NVIDIA/NeMo-Guardrails/tree/main/examples/grounding_rail). The chatbot will be asked several questions about a certain report, and we will use Guardrails to prevent it from answering facts that are not contained within the document. We will implement this rail using both OpenAI and Llama2 models.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./docs/imgs/fact_checking_workflow.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load environment \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Guardrails configuration files located under `fact_check_config/openai` erasing the `fact_check.co` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiaperez/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/openai\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked some questions about the document contained in the `fact_check_config/openai/kb` folder, it answers accurately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the US Bureau of Labor Statistics, the unemployment rate in March 2023 was 3.5 percent.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"What was the unemployment rate in March 2023?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the US Bureau of Labor Statistics, the unemployment rate for teenagers in March 2023 was 9.8 percent.\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for teenagers?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked a question that is not covered by the document, it gives a false statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the US Bureau of Labor Statistics, the unemployment rate for senior citizens in March 2023 was 4.7 percent.\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for senior citizens?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the fact check rail\n",
    "\n",
    "By adding the `fact_check.co` file back in the configuration, we will prevent the chatbot from hallucinating facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/openai\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked some questions about the document contained in the `fact_check_config/openai/kb` folder, it answers accurately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the March 2023 US jobs report, the unemployment rate in March 2023 was 3.5 percent.\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\", \"content\": \"What was the unemployment rate in March 2023?\"}]\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the March 2023 US jobs report, the unemployment rate for teenagers was 9.8 percent.\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for teenagers?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked a question that is not covered by the document, it effectively replies that it is no enough information to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough information to answer\n"
     ]
    }
   ],
   "source": [
    "history.append(bot_message)\n",
    "history.append(\n",
    "    {\"role\": \"user\", \"content\": \"What was the unemployment rate for senior citizens?\"}\n",
    ")\n",
    "bot_message = await app.generate_async(messages=history)\n",
    "print(bot_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the HuggingFace model and create a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiaperez/anaconda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/sofiaperez/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    logprobs=None,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the pipeline using Langchain HuggingFacePipeline class. Then wrap it again using Nemo’s get_llm_instance_wrapper function and register it using register_llm_provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Guardrails configuration files located under `fact_check_config/llama2` erasing the `fact_check.co` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked some questions about the document contained in the `fact_check_config/llama2/kb` folder, it answers accurately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the latest US jobs report, the unemployment rate in March 2023 was 3.5 percent.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate in March 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the most recent data from the Bureau of Labor Statistics, the unemployment rate for seniors (aged 65 and older) was 3.2% in March 2023. However, I must inform you that this figure may not be accurate due to various factors such as underreporting or misclassification of workers. If you have any further questions or concerns, please feel free to ask.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate for senior citizens?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the fact check rail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the `fact_check.co` file back in the configuration to prevent the chatbot from hallucinating facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked some questions about the document contained in the `fact_check_config/llama2/kb`, no answer is provided. Something is not working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate in March 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the fact check rail + few shot in prompt\n",
    "\n",
    "By adding some few shot examples to the prompt in the `general.yml` file located in the library `nemoguardrails/llm/prompts/` folder, the rail starts working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "HFPipeline = get_llm_instance_wrapper(\n",
    "    llm_instance=llm, llm_type=\"hf_pipeline_llama2\"\n",
    ")\n",
    "\n",
    "register_llm_provider(\"hf_pipeline_llama2\", HFPipeline)\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_path(\"./fact_check_config/llama2\")\n",
    "\n",
    "# create rails\n",
    "app = LLMRails(config, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When asked a question that is not covered by the document, it effectively replies that it is no enough information to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for WrapperLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await app.generate_async(prompt=\"What was the unemployment rate for senior citizens?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
